# PipeLM ğŸš€

*A lightweight, modular tool for running Large Language Models (LLMs) from Hugging Face.*

PipeLM provides an intuitive CLI interface for interactive chat and a robust FastAPI server to integrate LLMs seamlessly into your applications.

---

## âœ¨ Overview

PipeLM simplifies interaction with AI models, allowing you to:

- ğŸ“¥ **Download and manage** models from Hugging Face.
- ğŸŒ **Serve models** through a standardized REST API.
- ğŸ’¬ **Test prompts** via an interactive chat interface.
- ğŸ“œ **Maintain conversation history**.
- ğŸ”„ **Easily switch models** with minimal configuration changes.

---

## ğŸŒŸ Features

- ğŸ–¥ï¸ **Interactive CLI Chat**: Engage directly from your terminal.
- ğŸš€ **FastAPI Server**: REST APIs with built-in health monitoring.
- ğŸ§© **Efficient Model Management**: Download and manage models easily.
- ğŸ“¦ **Docker Support**: Containerize your models for better isolation.
- âš¡ **GPU Acceleration**: Automatically utilize available GPUs.
- ğŸ¯ **Model Quantization**: Reduce memory usage (4-bit and 8-bit).
- ğŸ“š **Conversation History**: Persistent chat context.
- ğŸ’¡ **Rich Terminal Interface**: Enhanced CLI with markdown rendering.
- âœ… **Robust Error Handling**: Graceful handling of issues.

---

## ğŸ› ï¸ Installation

### ğŸ“¦ From PyPI (Recommended)
```bash
pip install pipelm
```

### ğŸ’» From Source
```bash
git clone https://github.com/yourusername/pipelm.git
cd pipelm
pip install -e .
```

### ğŸ³ With Docker
```bash
git clone https://github.com/yourusername/pipelm.git
cd pipelm

docker build -f docker/Dockerfile -t pipelm .

docker run -p 8080:8080 -v pipelm_data:/root/.pipelm -e HF_TOKEN=your_token -e MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2 pipelm
```

---

## ğŸš¦ Usage

### ğŸ“¥ Download a Model
```bash
pipelm download mistralai/Mistral-7B-Instruct-v0.2
```

### ğŸ“‹ List Downloaded Models
```bash
pipelm list
```

### ğŸ’¬ Interactive Chat
```bash
pipelm chat mistralai/Mistral-7B-Instruct-v0.2

# Using local model
pipelm chat /path/to/local/model

# With quantization
pipelm chat mistralai/Mistral-7B-Instruct-v0.2 --quantize 4bit
```

### ğŸš€ Start API Server
```bash
pipelm server mistralai/Mistral-7B-Instruct-v0.2 --port 8080

# Using local model
pipelm server /path/to/local/model --port 8080

# With quantization
pipelm server mistralai/Mistral-7B-Instruct-v0.2 --quantize 8bit
```

### ğŸ³ Docker Compose
```bash
export HF_TOKEN=your_token
docker-compose up -d pipelm
```

---

## ğŸ“¡ API Endpoints

### ğŸ› ï¸ Quick Commands

#### Check Server Health:
```bash
curl http://localhost:8080/health
```

#### Send a Sample Prompt:
```bash
curl -X POST http://localhost:8080/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Explain the difference between AI and machine learning."}],
    "max_tokens": 200,
    "temperature": 0.7,
    "top_p": 0.9
  }'
```


### âœ… GET `/health`
Health status of server and model.

```json
{
  "status": "healthy",
  "model": "Mistral-7B-Instruct-v0.2",
  "uptime": 42.5
}
```

### ğŸ“– GET `/`
Swagger UI for API documentation.

### âœï¸ POST `/generate`
Generate text from conversation history.

Request:
```json
{
  "messages": [
    {"role": "user", "content": "What is artificial intelligence?"}
  ],
  "max_tokens": 1024,
  "temperature": 0.7,
  "top_p": 0.9
}
```

Response:
```json
{
  "generated_text": "Artificial intelligence (AI) refers to the simulation of human intelligence in machines..."
}
```

---

## ğŸ¯ Chat Commands

- `/exit` or `/quit` â€“ Exit chat
- `/clear` â€“ Clear conversation history
- `/info` â€“ Display current model information

---

## âš™ï¸ Environment Variables

- `HF_TOKEN`: Your Hugging Face token (required).
- `MODEL_DIR`: Local model directory.
- `PORT`: Server port (default: 8080).

---

## ğŸ“ Project Structure
```
pipelm/
â”œâ”€â”€ pipelm/                 # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ downloader.py
â”‚   â”œâ”€â”€ chat.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ docker/                 # Docker setup
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âœ… Requirements

- Python 3.8+
- Torch (GPU support recommended)
- 16+ GB RAM (model-dependent)
- CUDA-compatible GPU (recommended)

---

## ğŸš© Troubleshooting

### Model Download Issues
- Verify Hugging Face token.
- Check network connectivity.

### Server Startup Issues
- Change default port if already in use.
- Ensure dependencies are installed.

### Memory Issues
- Use smaller models or quantization.

---

## ğŸ’½ Model Storage

- Linux/Mac: `~/.pipelm/models/`
- Windows: `%LOCALAPPDATA%\pipelm\models\`
- Docker: `/root/.pipelm/models/`

---

## ğŸ™Œ Contributing

Contributions are welcome! Submit a Pull Request.

---

## ğŸ“œ License

MIT License. See `LICENSE` for details.
